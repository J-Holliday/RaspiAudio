\documentclass{jsarticle}
\usepackage{multicol}

% sourcecode highlight
\usepackage{color}
\usepackage{listings,jlisting}

%jlisting (sourcecode highlight)
\lstset{
language={C},
backgroundcolor={\color[gray]{.85}},
basicstyle={\small},
identifierstyle={\small},
commentstyle={\small\ttfamily \color[rgb]{0,0.5,0}},
keywordstyle={\small \color[rgb]{0,0,0}},
ndkeywordstyle={\small},
stringstyle={\small\ttfamily \color[rgb]{0,0,1}},
frame={tb},
breaklines=true,
columns=[l]{fullflexible},
numbers=left,
xrightmargin=0zw,
xleftmargin=3zw,
numberstyle={\scriptsize},
stepnumber=1,
numbersep=1zw,
morecomment=[l]{//}
}

\title{音声で動作するオーディオプレイヤの製作}
\author{14EC004\ 飯田頌平}

\begin{document}
\maketitle

%\section*{目次}
%\label{sec:目次}
%\label{目次}

%\begin{tabular}{rrl}
%第\ref{sec:序文}章 & p.\pageref{序文} & 序文 \\
%第\ref{sec:概要}章 & p.\pageref{概要} & 概要 \\
%第\ref{sec:外部仕様}章 & p.\pageref{外部仕様} & 外部仕様 \\
%第\ref{sec:内部仕様}章 & p.\pageref{内部仕様} & 内部仕様 \\
%第\ref{sec:今後の展望}章 & p.\pageref{今後の展望} & 今後の展望
%\end{tabular}

\begin{multicols}{2}

\section*{目次}
\label{sec:目次}
\label{目次}
\begin{tabular}{rrl}
第\ref{sec:序文}章 & p.\pageref{序文} & 序文 \\
第\ref{sec:概要}章 & p.\pageref{概要} & 概要 \\
第\ref{sec:外部仕様}章 & p.\pageref{外部仕様} & 外部仕様 \\
第\ref{sec:内部仕様}章 & p.\pageref{内部仕様} & 内部仕様 \\
第\ref{sec:今後の展望}章 & p.\pageref{今後の展望} & 今後の展望
\end{tabular}

\section{序文}
\label{sec:序文}
\label{序文}

音声認識システムは日々進歩している．
組込み機器に音声による対話システムを組み込むことはいまや珍しいことではなくなり，
それどころか人との自然な会話でさえある程度こなすロボットまでもが登場しているのだ．
Siri\footnote{Apple社が開発したiPhone用音声認識システム}などは，
音声認識が一般人にも身近な技術となったことの好例である．

そのような時代の中，わずか数インチのコンピュータRaspberry\ Piが発売された．
Raspberry\ PiはUNIXベースのディストリビューションに対応しており，Pythonによって動かすことができる．
Pythonの汎用性は素晴らしく，GPIOピンの操作からCGIの作成，果ては科学的な計算にも対応しており，
フロントエンドにおける大抵のユーザの要求に応える能力を持つ．

このPythonとLinux向けに開発されたソフトウェア（音声認識ソフトや音声合成ソフト）を活用することによって，
Raspberry\ Piを音声で動かすことができる．
またRaspberry\ Piは携帯性に優れたサイズであり，Linux標準のwavファイル再生コマンドなどを活用すれば，
ウォークマンを模したオーディオプレイヤをRaspberry\ Piによって再現することもできる．
既存のPCはボードのサイズが比較的大きく，ウォークマンのようにポケットに入れることはできなかった．

そうした分野は専ら組込みの分野に独占されており，
汎用性を持つPCでありながらポケットサイズになったRaspberry\ Piの登場によって，
Siriのような多機能の音声認識デバイスをアマチュアでも気軽に作成することができるようになった．
そこで音声認識で動作するオーディオプレイヤを製作しようというのが本レポートの主題である．
無論ここでいうオーディオプレイヤとは単なる音楽の再生をするだけのものではなく，
iPodのように汎用的な機能を持ったデバイスを指す．

本レポートの第2章では構成や制作環境について簡潔にまとめている．
また第3章では作成するオーディオの使用方法（外部仕様）について示す．
製作物の概要について知りたい場合は，ここまでの章を読むだけで良い．

第4章では採用したアルゴリズムの説明や，ソースコードの解説を行う．
https://github.com/J-Holliday/RaspiAudio
上に置かれたソースコードと併用することで，内部の仕組みについて理解することができるだろう．

最後に第5章において，オーディオプレイヤの現時点での性能上の問題や，その改善点について示す．

\newpage

\section{概要}
\label{sec:概要}
\label{概要}

本レポートで作成するオーディオプレイヤは以下の部品で構成されている．
\begin{itemize}
\item Raspberry\ Pi
\item Raspberry\ Pi用電源
\item 音声入力用マイク
\item 音声出力用ヘッドフォン
\item 無線子機
\item 音声認識サーバ
\end{itemize}

サーバは外付けの物を利用し，ネットワークを経由してRaspberry\ Piと通信する．
認識サーバについてはRaspberry\ Piのバックグラウンドで動作させることも可能であるが，
サーバにはある程度の処理能力が要求される．
その要求される能力は認識させたい単語の数に比例し，
自然言語処理の域まで単語を詳細に分解しようと言うのであれば，
Raspberry\ Piのスペックでは到底追いつかない．
一方数十語程度しか単語を認識させないのであれば，別途サーバを用意する必要はない．

次にプログラムの動作環境を示す．
\newline \\ 
\begin{tabular}{lcl}
\centering
PC & : & Raspberry\ Pi2 \\
PCのOS & : & Raspbian\ -\ Jessie \\
音声認識エンジン & : & Julius\ -\ 4..3.1 \\
認識用モデル & : & dictation-kit-v4.3.1-linux \\
認識サーバ & : & 実機またはVirtualBox \\
サーバのOS & : & Ubuntu\ 14.04 \\
フレームワーク & : & CherryPy-3.8.1 \\
音声合成ソフト & : & aquestalkpi \\
スクリプト & : & Python2,\ sh \\
\end{tabular}
\newline

プログラムを実行するに辺り，事前に以上の動作環境を用意しておく必要がある．
冗長かつ本質から逸れるためにインストール手順等については触れないが，
Web上の情報を参考にすれば誰でも無償で用意できるものばかりである．

また一部ソフトウェアの，デフォルトの解凍先ディレクトリについて明示しておく．
\begin{center}
/home/pi/workspace/raspi-audio/downloads/
\end{center}
ただし認識用モデルについては，以下のディレクトリに解凍すること．
\begin{center}
/home/pi/workspace/raspi-audio/downloads/julius-kits/
\end{center}

そして以下のディレクトリに楽曲ファイルを用意する．
\begin{center}
/home/pi/workspace/raspi-audio/music/
\end{center}
楽曲ファイルについてはtokyoのbranchにあるRaspiAudio/music/を参考にするとよい．
将来的に楽曲ファイルの追加とファイルサイズの肥大化が見込まれるため
現行ブランチからはRaspiAudio/music/を削除しているので，注意すること．

Pythonのモジュールについても適宜インストールしておかなければならない．
ソースコードを参考にして，未インストールのモジュールをインストールする．
また使用しているPythonのバージョンが3である場合は，以下のコマンドでPython2へと一時的にダウングレードできるので，活用すると良い．

\begin{lstlisting}[caption=/bin/sh]
# temporary downgrade
$ virtualenv -p /usr/bin/python2.7 --distribute temp-python
$ source temp-python/bin/activate

# upgrade again
$ deactivate
\end{lstlisting}

最後にディレクトリ構成について示す．
最上位のディレクトリをRaspiAudioとする．
オーディオプレイヤとして機能させるためには，
RaspiAudioの直下に最低でも以下の構成要素が必要である．
\begin{itemize}
\item ASRServer.py
\item musicPlayer/
\item semanticAnalysys/
\item raspisan.py
\item voice/
\item yukkuri/
\end{itemize}

branchによっては要件を満たさないものもあるため，
fetchまたはpullしたディレクトリについて注意を払わなければならない．
masterの構成が要件を満たしているとも限らないので，
要件を満たすbranchにcheckoutしなければならない．

\section{外部仕様}
\label{sec:外部仕様}
\label{外部仕様}

この章ではオーディオプレイヤの使用方法について説明する．
口頭で命令を述べるだけで動作するのだが，
その前準備として幾らかの手順を踏む必用があるため，
主にその部分について述べる．

\subsection{プログラムのインストール}

プログラムはgithubに公開されている．
以下のコマンドによって作業用ディレクトリに保存する．
\begin{lstlisting}[caption=/bin/sh]
$ git clone https://github.com/J-Holliday/RaspiAudio
\end{lstlisting}
gitコマンドが実行されない場合には，gitをインストールする必要がある．
Raspberry\ Piにgitをインストールするには
\begin{lstlisting}[caption=/bin/sh]
$ sudo apt-get install git
\end{lstlisting}
を実行すればよい．
なお，デフォルトの作業用ディレクトリは
\begin{center}
/home/pi/workspace/raspi-audio/
\end{center}
である．任意のディレクトリを指定する場合は，
ファイルパスの構成に気を付ける必要がある．

cloneに成功したら，branchを切り替える必要がある．
2016年2月16日現在，masterでは前述の要件を満たしていない．
この時点で要件を満たすbranchはtakaoである．
よって以下のコマンドでbranchを変更する．
\begin{lstlisting}[caption=/bin/sh]
$ git checkout -b takao origin/takao
\end{lstlisting}
これでプログラムのインストールは完了した．

\subsection{基本的な使い方}

オーディオプレイヤの基本的な使い方は以下の通りである．
\begin{enumerate}
\item 音声認識サーバを起動する
\item 実行プログラムを起動する
\item マイクに向かって発話する
\end{enumerate}

音声認識サーバは以下のコマンドで起動する．
\begin{lstlisting}[caption=/bin/sh]
$ python ASRServer.py
\end{lstlisting}
すると8000番ポートが待ち受け状態となり，
ここにwavファイルをPOSTで送信することで，
音声認識サーバたるJuliusへwavファイルが入力されるようになる．
なお，ASRServer.pyはsayonariによるスクリプト\cite{sayonari}を改変したものである．

次いで実行プログラムを起動する．
\begin{lstlisting}[caption=/bin/sh]
$ python raspisan.py
\end{lstlisting}

ヘッドフォンから合成音声による挨拶が聞こえたら，
プログラムが発話を待ち受ける状態へと遷移する．
試しに「音楽を再生して下さい」などと喋りかけて見ると，
Raspberry\ Piから音楽が再生されるようになる．

他にも複数のRaspberry\ Piが解釈できる命令が搭載されている．
命令のうち代表的なものを抜粋し次節に示す．

\subsection{代表的な命令}

当オーディオプレイヤはただの音楽再生機器としての命令だけではなく，
linuxコンピュータとしての命令をも実行することができる．

\begin{itemize}
\item 音楽を再生する
\item 音楽を停止する
\item 楽曲ファイルを選択する
\item シャッフル再生を行う
\item ニュースを流す
\item メールチェックを行う
\item アラームをセットする
\end{itemize}

命令文には特定の文法規則が必要となるが，
ある程度自然な会話でも解釈できるような仕組みを搭載しているため，
適当に喋りかけてみれば良い．


\section{内部仕様}
\label{sec:内部仕様}
\label{内部仕様}

\subsection{Juliusによる音声認識}

音声認識はJuliusによって行われる．
Juliusについての詳細な説明は開発者である李と河原のドキュメント\cite{kawahara}を参照すれば良いが，
動作原理の概要についてもここで触れておく．

Juliusは音響モデルと言語モデルとのパターンマッチによって入力音声から言語を抽出する音声認識エンジンである．
音響モデルとは音素や音節の周波数のパターンを指し，
言語モデルは音素に対応した語句や，語句を文としてまとめたデータを指す．
文については正規文法しか処理することが出来ない．

Juliusは幾つかの入力方式を持っており，今回はwavファイルを入力する方式を使用する．
単体で使う場合はサーバ側に専用のインタープリタが立ち上がり，クライアントから参照できないため，
PythonのフレームワークであるCherryPyを活用し，クライアントからPOSTでデータを送信できるようにする．

これらの設定はすべてASRServer.pyで設定している．
基となったコードの詳細は製作者のサイト\cite{sayonari}に示されているため，
ここではオーディオプレイヤ用に変更した部分について説明する．

JULIUS\_HOMEおよびJULIUS\_EXECはそれぞれ前述の解凍先ディレクトリ内を参照している．
SERVER\_PORTは8000番としているが，
PythonのSimpleHTTPServerなどと競合しやすいポート番号であるので，必要に応じて変更すること．
ASR\_FILEPATHはRaspiAudio/voiceを指定している．
voice/がないbranchで実行すると，無論エラーとなるので，気を付ける．

JuliusはJulius\_EXECが指定したスクリプトをsubprocess.Popenで子プロセスとして実行することによって実行される．
subprocess.Popenは引数のシェルスクリプトを実行するコマンドであり，
そこからJuliusを実行する際のパラメータの指定方法が伺える．
Juliusのパラメータはバイナリの後に-Cオプションで設定ファイルを指定している．
設定ファイルの中身はパラメータが記載されており，ここで音声モデルと言語モデル，辞書ファイル，入出力方式を設定する必要がある．

クライアント側における入出力の遣り取りはraspisan.py内で行われる．
以下にJuliusへリクエストを送信しレスポンスを受け取るまでのコードを抜粋して示す．
\begin{lstlisting}[caption=RaspiAudio/raspisan.py]
url = ``http://192.168.100.101:8000/asr_julius''
files = { 
  `myFile': open(`voice/record.wav','rb')
}
s = requests.Session()
r = s.post(url, files=files)
\end{lstlisting}

まずはrequestsの属性Session()によってセッションのインスタンスを作成している．
ここで名前空間requestsはpythonのモジュールである．
Session()のインスタンス変数sの属性postは，
第一引数に送信先URLを，第二引数に送信先データを指定してPOSTを実行するメソッドである．
URLのドメイン（IP）部分にはJuliusサーバ（CherryPy）のドメイン（IP）及びSERVER\_PORTで指定したポート番号を指定する．
パスで指定するasr\_juliusという値は，クラスASRServerの関数asr\_juliusを呼び起こす．

\begin{lstlisting}[caption=RaspiAudio/ASRServer.py]
class ASRServer(object):
  （省略）
  def asr_julius(self, myFile):
    （省略)
\end{lstlisting}

ASRServer.pyのソースを見るとasr\_juliusの引数が呼び出し元の名前空間とmyFileという仮引数であることがわかるが，
この仮引数の名前がfilesで指定したdict形式のデータ構造のキーに結びついている．
そのためリードバイナリ形式で開かれたvoice/record.wavという音声ファイルがASRServerへと渡される．
ASRServerはJuliusの実行コマンドJULIUS\_EXECを引数に持つインスタンス変数pをクラス変数として定義しており，
asr\_julius内でpを呼ぶことでJuliusに音声ファイルを入力している．
Juliusからの応答はraspisan.py内のrへと返される．
この応答が持つ属性の中に，認識された文章（単語列）が含まれているのである．

\subsection{認識された文章の解釈}

Juliusからのレスポンスによって，音声認識の結果の文章を得ることができる．
それでは得た文章をどのようにしてプログラムに解釈させればよいのか．

文章解釈プログラムは
RaspiAudio/semanticAnalysis/semanticAnalysis.py
である．このモジュールはRaspiAudio/raspisan.pyから以下のように呼ばれる．

\begin{lstlisting}[caption=RaspiAudio/raspisan.py]
from semanticAnalysis import semanticAnalysis as sa
（省略）
r = s.post(url, files=files)
ary = r.text.split("\n")
data = ary[0].split(":")
words = data[1].strip(" ").split(" ")
on = sa.send(words)
\end{lstlisting}

Juliusからのレスポンスをパースして文字列のリストであるwordsに格納し，
それを引数に取ってsemanticAnalysisモジュールの関数send()を呼ぶ．
send()は引数から特徴ベクトルを検出して最適だと思われる命令を選択し，onへと命令のキーを返す．

特徴ベクトルの検出は以下の順で実行される．
\begin{enumerate}
\item 単語データと命令データを読み込む
\item 語彙抽出を行い，入力文章の全体の特徴ベクトルを検出する
\item 命令データから命令のモデルを作成する
\item 命令の特徴ベクトルとパターンマッチを行い，最小誤差のモデルを選ぶ
\item モデルから命令のキーを取得し，呼び出し元に返す
\end{enumerate}

単語データの読み込みはソースコード\ref{csv}によって行われる．
ここではCSV形式の単語データを二次元の配列worditemに格納している．
worditemはいずれのクラス・関数にも属さないモジュール直下の変数であり，
イミュータブルでありながら参照することが出来る．
同様にして命令データをorderitemに束縛している．

\begin{lstlisting}[caption=RaspiAudio/semanticAnalysis/semanticAnalysis.py,label=csv]
# get data from word.csv
f = open("semanticAnalysis/word.csv")
res = f.read()
f.close()
line = res.split("\n")
worditem = []
for i in range(1,len(line)-1):
  worditem.append(line[i].split(","))
\end{lstlisting}

語彙抽出の方法は単純である．
文字列リストを拡張for文に入れて，各要素に対しworditemの要素と一致するか否かを見ればよい．
このとき一致した場合，特徴ベクトルの値を更新する．
特徴ベクトルの扱いに関しては，モジュールsemanticAnalysis下のクラスfeatureVectorによって行われている．
更新する場合，ベクトルを引数にしてfeatureVector.setVector()を呼ぶ．
このときデフォルトで重みを1だと決めているのだが，オプションで重みを変更することも出来る．
featureVector.setVector()の処理をソースコード\ref{setVector}に示す．

\begin{lstlisting}[caption=RaspiAudio/semanticAnalysis/semanticAnalysis.py,label=setVector]
def setVector(self, coordinate, weight=1, vectype="input"):
  """set feature vector include init."""
  try:
    # select vector
    if vectype == "input":
      v = featureVector.fv
    elif vectype == "model":
      v = np.arange(0)
    # init vector
    if len(v) == 0:
      v = featureVector.initVector()
    # set value
    column = coordinate % 10
    row = (coordinate - column)/10
    v[row][column] += weight
    # update
    if vectype == "input":
      featureVector.fv = v
    elif vectype == "model":
      featureVector.model.append(v)
  except:
    print "Exception in setVector."
    print "coordinate:%d, weight=%d" % (coordinate, reward)
\end{lstlisting}

featureVector.setVector()では10*10の行列をベクトル空間のモデルとし，
ベクトルを引数coordinateとしてその座標上に捉えて重み付けをしている．

行列はリストのクラス変数featureVector.fvで定義されているが，
featureVector.fvは初期状態ではnumpy.array型の空の行列である．
featureVector.setVector()が呼ばれた際に行列が空であった場合は，
featureVector.initVector()を実行して初期化処理を行う．
初期化処理によって，すべての成分の値が0である10*10の行列を得られる．

なお，オプションvectypeにmodelを指定したときは，
入力文章の特徴ベクトルを指すfeatureVector.fvではなく，
命令モデルの特徴ベクトルを指すfeatureVector.modelが対象の行列となって上記の処理が行われる．
ただしfeatureVector.fvが行列であるのに対し，featureVector.modelは行列のリストである．
すべての命令モデルの行列がfeatureVector.modelに格納されているのだ．

入力値と命令モデルの誤差を検出する方法は，以下の式によって示される．
\begin{equation}
\label{eq:error}
\min { \sum _{ i=0 }^{ l }{ \left| { x }_{ i }-{ y }_{ i } \right|  }  } 
\end{equation}

{\bf x}は命令モデルの特徴ベクトル，{\bf y}は入力文の特徴ベクトル，
lは特徴ベクトルの系列長である．

式\ref{eq:error}を実現するのにはループ処理を使えば良いが，
ここでは式\ref{eq:errorv}のようにベクトルの内積を取ることで簡潔にまとめた．
\begin{equation}
\label{eq:errorv}
\min { { \o  }^{ T }\left| \overrightarrow { x } -\overrightarrow { y }  \right| \o  } 
\end{equation}

なお$\o$はすべての成分が1である1*10行列である．
numpy.dot()を使うと，第一引数と第二引数の内積を求めることができる．
内積を取ることによって，100次元のデータdfを1次元のデータsummationに圧縮し，
summationの大小を比較することによって最小誤差のモデルを判断している．
なお，dfは命令モデルと入力文の特徴ベクトルの差である．

以上の処理の実装をソースコード\ref{collation}に示す．

\begin{lstlisting}[caption=RaspiAudio/semanticAnalysis/semanticAnalysis.py,label=collation]
import numpy as np
（省略）
def collation(self):
  """collate model and feature vector."""
  try:
    # init model
    if len(featureVector.model) == 0:
      print "init model"
      featureVector.setModel()
    # search error between model and feature vector
    candidate = featureVector.model[0], 9999 # model, df-value
    for model in featureVector.model:
      df = model - featureVector.fv
      ones = np.repeat(np.array([1]), 10) # (1, 1, 1, ... , 1)
      summation = np.dot(np.abs(df), ones).dot(ones)
      # update candidate
      if summation < candidate[1]:
        candidate = model, summation 
    print candidate
    return candidate[0]
  except:
    print("Exception in collation.")
\end{lstlisting}

最後に，得られた命令モデルから命令のキーを取得する．
命令とモデルとキーはそれぞれ１対１の関係にあるため，探索を掛けることで命令のキーを得ることが出来る．

\subsection{フィルタ処理}

音声認識を行う上では，周囲の雑音の対策をする必要があるのは自明のことだ．
雑音はパターンマッチの精度を下げる要因になるためである．

Juliusでは周波数によるパターンマッチが行われている．
そのため雑音による影響も，周波数成分上で表現されると言える．
故に周波数に関してフィルタ処理を施せば，雑音を抑えることが出来るかもしれない．

このことは荒木によるテキスト\cite{araki}でも言及されている．
加法性雑音と呼ばれる背景雑音については，周波数空間での減算によって消去可能なのだ．

Juliusでは単純な減算（supectrul\ subtraction）より優秀なケプストラム平均減算による雑音除去が行われているが，
河原らのドキュメント\cite{kawahara2}によるとマイク入力時における記述しかない．
調査不足の可能性はあるが，wavファイル入力時における雑音除去は為されていないと仮定し，
単純な減算フィルタを用意することにした．

フィルタのプログラムは
RaspiAudio/voice/lowpassFilter.py
である．
このプログラムはaidiaryによるプログラム\cite{aidiary}を改変したものである．
まずは改変したプログラムをソースコード\ref{filter}に示す．

\begin{lstlisting}[caption=RaspiAudio/voice/lowpassFilter.py,label=filter]
#coding:utf-8
import struct
import wave
import numpy as np
import scipy.signal
from pylab import *

def save(data, fs, bit, filename):
  wf = wave.open(filename, "w")
  wf.setnchannels(1)
  wf.setsampwidth(bit / 8)
  wf.setframerate(fs)
  wf.writeframes(data)
  wf.close()

def filtering():
  wf = wave.open("voice/record.wav", "r")
  fs = wf.getframerate()

  x = wf.readframes(wf.getnframes())
  x = frombuffer(x, dtype="int16") / 32768.0

  nyq = fs / 2.0  # nyquist frequency

  # make filter
  # normalize nyquist frequency
  fe = 7500.0 / nyq  # cutoff frequency
  numtaps = 255      # number of taps

  b = scipy.signal.firwin(numtaps, fe) # Low-pass

  # filtering
  y = scipy.signal.lfilter(b, 1, x)

  # output filtered data
  y = [int(v * 32767.0) for v in y]
  y = struct.pack("h" * len(y), *y)
  save(y, fs, 16, "whitenoise_filtered.wav")
\end{lstlisting}

このモジュールを活用するには，lowpassFileter.filtering()を外部から呼び出せば良い．
するとまずはwave.open()により録音された音声ファイルrecord.wavが開かれる．
第二引数にrを指定したことにより，返り値wfはWave\_readオブジェクトとなる．

Wave\_readオブジェクトの属性getframerate()はサンプリング周波数を返す．
サンプリング周波数fsを2で割った値はナイキスト周波数としてnyqに代入される．

Wave\_read.getnframes()はオーディオフレームの総数を返す．
オーディオフレームとはサンプリングされたデータのことであり，
すなわちこのメソッドはwavファイルがサンプリングされた回数の合計値を返す．
この合計値とWave\_read.readframes()を活用することで，
wavファイル全体の標本値の列を得ることが出来る．
Wave\_read.readframes()は数値型の引数をひとつ取り，
その値が示す個数のオーディオフレーム値をバイナリ文字列で返すメソッドである．
こうして得たオーディオフレームのバイナリxを，np.frombuffer()によって数値のarrayに変換している．

scipy.signal.firwin()は，窓関数を用いたFIRフィルタを作成する．
その第一引数はフィルタ係数であり，第二引数はカットオフ周波数である．
カットオフ周波数feの実装は，フィルタのカットオフ周波数をナイキスト周波数で正規化することによって得られているので，
フィルタのカットオフ周波数（ここでは7500）を変更することでフィルタの帯域を変更することが出来る．

そしてscipy.signal.lfilter()によってフィルタリングが実行される．
最後にその実行結果yを音声バイナリに戻して保存している．

\section{今後の展望}
\label{sec:今後の展望}
\label{今後の展望}

音声認識オーディオプレイヤの肝となるのは，やはり音声認識である．
Juliusでは予め辞書ファイルに登録された正規文法しか処理できない上，
登録された語句が多ければ多いほど誤検知も増えるといった問題を抱えている．
この点を近年発達の著しい機械学習を活用することで改善できるのではないか．

当初の計画ではN-gramモデルに立脚した選択アルゴリズムや，
Webサーバからコーパスを取得するプログラムのような機械学習の仕組みを
オーディオプレイヤに搭載する予定であったのだが，
時間的・技術的問題により断念せざるを得なくなった．
この失敗を活かし，克服することができれば
一挙にオーディオプレイヤの使い勝手が向上することだろう．

また，視覚的なインタフェースに対応させることも視野にいれるべきだ．
現状のオーディオプレイヤでは完全に音声による入出力に限定されている．
しかしより優れた対話型デバイスを目指すというのであれば，
それはより人間に近づく進化が必要だろう．
単なるGUIへの対応だけではなく，
MMDAgent\footnote{http://mmdagent.jp}のような3次元CGで作成されたキャラクターを音声認識で動かす技術を実装することが出来るなら，
現在よりもずっと人間との対話に近づくに違いない．
個人が趣味として行うにはまだまだ難しいとは思うが，
いずれこのような人と機械との対話を実現させてみたいと思う．

\begin{thebibliography}{9}
\bibitem{sayonari}
	sayonari．Juliusで音声認識サーバを立てて，wavファイルをPOST送信して認識する．\\
	http://qiita.com/sayonari/items/65a5aea83d1fadac7d5c
\bibitem{kawahara}
	李 晃伸， 河原 達也．
	"Julius を用いた音声認識インタフェースの作成"．
	ヒューマンインタフェース学会誌， Vol．11， No．1， pp．31--38， 2009．
\bibitem{kawahara2}
	河原 達也， 李 晃伸．
	「連続音声認識ソフトウエア Julius」
	人工知能学会誌， Vol．20， No．1， pp．41--49， 2005．
\bibitem{araki}
	荒木 雅弘．
	イラストで学ぶ音声認識．
	講談社，2015．
\bibitem{aidiary}
	aidiary．
	SciPyのFIRフィルタの使い方． \\
	http://aidiary.hatenablog.com/entry/20111102/1320241544
\end{thebibliography}


\end{multicols}
\end{document}